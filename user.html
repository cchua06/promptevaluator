<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>GenAI Prompt Feedback POC</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script crossorigin src="https://unpkg.com/react@18/umd/react.development.js"></script>
    <script crossorigin src="https://unpkg.com/react-dom@18/umd/react-dom.development.js"></script>
    <script src="https://unpkg.com/@babel/standalone/babel.min.js"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
    <style>
      body { @apply bg-gray-50 text-gray-800; font-family: 'Inter', sans-serif; }
      textarea { font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace; }
      .fade-in { animation: fade-in 0.3s ease-out; }
      @keyframes fade-in { from { opacity:0; transform: translateY(4px);} to { opacity:1; transform: translateY(0);} }
      h1, h2, h3, h4, h5, h6, .font-bold, .font-semibold, .text-3xl, .text-4xl, .font-bold, .font-semibold {
        font-family: 'Inter', sans-serif;
        font-weight: 600;
      }
    </style>
  </head>
  <body class="min-h-screen flex flex-col">
    <div id="root" class="flex-1"></div>

    <script type="text/babel">
      // ---------------------------
      // OpenAI Evaluator Function
      // ---------------------------

      async function evaluatePrompt(prompt) {
        // System instructions for the evaluator
        const systemInstructions = `
        📥 SYSTEM PROMPT: Prompt Structure Evaluator

        You are a prompt reviewer. Your task is to evaluate whether a user’s prompt includes all 4 required components:

        Task: Clear action with a specific deliverable and audience.

        Input: Raw materials, context, or data sources the AI should use.

        Steps: A numbered, sequential thought process (with action verbs).

        Output: Clear format, tone, and structural expectations.

        🔍 Instructions:
        Parse the user’s prompt carefully.

        For each component, mark as:

        ✅ Present

        ❌ Missing
        If present but weak (vague, incomplete, or lacking best practices), flag it.

        Then produce a brief structured summary using the appropriate output format.

        📋 Output Format:
        If Failed:

        Let’s try again! 🤓

        Missing components: [List any of: Task, Input, Steps, Output]

        Improvements needed in: [List components that are present but need strengthening]

        Breakdown:
        Task: [If weak or missing, explain briefly why]
        Input: [If weak or missing, explain briefly why]
        Steps: [If weak or missing, explain briefly why]
        Output: [If weak or missing, explain briefly why]

        If Successful:

        Amazing job! 🎉

        All 4 components are present!

        Best practices demonstrated in: [List highlights such as “clearly scoped input”, “well-sequenced steps”, “precise output formatting”]

        ✅ Evaluation Criteria Recap:
        Task: Starts with a verb, includes a defined deliverable and target audience.

        Input: Specific and bounded; includes background, data, or cited sources.

        Steps: Numbered, uses action verbs, includes decisions or clarifications where needed.

        Output: Provides template-level clarity on tone, length, structure, and format.

        ⛔️ Do not suggest a revised prompt.
        ⛔️ Do not include a question or ask if the user wants help.
        ⛔️ Do not end with guidance or speculative improvements.
        ⛔️ Do not include any instruction text or headings in your output — only use the approved output format above.

        Keep your evaluation concise and focused—only highlight what matters most.
        `;

        try {
          const res = await fetch('/api/evaluate', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({ prompt, systemInstructions })
          });
          if (!res.ok) {
            throw new Error(`API error: ${res.status} ${res.statusText}`);
          }
          const data = await res.json();
          return { notes: data.notes };
        } catch (err) {
          console.error('Error evaluating prompt:', err);
          return { notes: 'There was an error contacting the evaluation service.' };
        }
      }

      const isBlank = (s) => !s || s.trim().length === 0;
      // Call backend to generate Facilitator Feedback using OpenAI GPT-4o
      const generateFacilitatorFeedback = async (prompt, evaluation) => {
        const formattedInput = `Prompt:\n${prompt}\n\nEvaluation:\n${evaluation}`;
        const systemInstructions = `
        Your task is to generate structured support content to help the facilitator guide the participant toward a better prompt.

        Inputs:
        Prompt:
        <prompt>

        Evaluation:
        <evaluation>

        Your output must follow this format:

        Task Summary:

        Summarize what the participant is trying to achieve based on their prompt. Use the evaluation only to clarify what’s missing or weak—don’t just restate it. Write 1–2 objective sentences. If the task is unclear, say so.

        Guidance:

        Write 2–5 bullet points that provide facilitator-facing guidance. These should be confident, actionable suggestions—not questions. Focus on moving the participant forward based on what’s wrong, as indicated in the evaluation. Avoid direct criticism or prompt rewrites.

        Be use-case specific when possible: e.g., suggest uploading a file, clarifying audience, giving example inputs, or breaking the task into steps.

        Always include this as the final bullet:
        - Ensure there's a source: an online link, uploaded file (e.g. agenda, program), or concise input directly in the prompt.
        `;

        try {
          const res = await fetch('/api/facilitator-feedback', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({ prompt, systemInstructions })
          });
          if (!res.ok) {
            throw new Error(`API error: ${res.status} ${res.statusText}`);
          }
          const data = await res.json();
          return data.feedback || '';
        } catch (err) {
          console.error('Error generating Facilitator Feedback:', err);
          return 'There was an error generating the facilitator feedback.';
        }
      };

      const sendToEvaluator = async ({ prompt }) => {
        const evaluationFeedback = await evaluatePrompt(prompt);
        const { notes } = evaluationFeedback;
        const facilitatorFeedback = await generateFacilitatorFeedback(prompt, evaluationFeedback.notes);
        return { notes, facilitatorFeedback };
      };


      // ---------------------------
      // Storage Toggle
      // ---------------------------
      let USE_LOCAL_STORAGE = false; // Will be set by backend

      const STORAGE_KEY = 'prompt_records_v1';

      function saveRecordLocal(rec) {
        const arr = JSON.parse(localStorage.getItem(STORAGE_KEY) || '[]');
        arr.unshift(rec);
        localStorage.setItem(STORAGE_KEY, JSON.stringify(arr));
      }
      function loadRecordsLocal() {
        try { return JSON.parse(localStorage.getItem(STORAGE_KEY) || '[]'); } catch { return []; }
      }

      // ---------------------------
      // React Component
      // ---------------------------
      function App() {
        const [firstName, setFirstName] = React.useState("");
        const [lastName,  setLastName]  = React.useState("");
        const [prompt,    setPrompt]    = React.useState("");
        const [records,   setRecords]   = React.useState([]);
        const [isSubmitting, setIsSubmitting] = React.useState(false);
        const [lastEval,  setLastEval]  = React.useState(null);
        // Facilitator Feedback is not shown to the user

        // On mount, get storage mode and load records if local
        React.useEffect(() => {
          fetch('/api/storage-mode').then(r => r.json()).then(({ useLocalStorage }) => {
            USE_LOCAL_STORAGE = useLocalStorage;
            if (USE_LOCAL_STORAGE) setRecords(loadRecordsLocal());
          });
        }, []);

        // For localStorage, keep in sync
        React.useEffect(() => {
          if (USE_LOCAL_STORAGE) localStorage.setItem(STORAGE_KEY, JSON.stringify(records));
        }, [records]);

        const handleSubmit = async (e) => {
          e.preventDefault();
          if (isBlank(firstName) || isBlank(lastName)) return alert("First and last name cannot be blank.");
          if (!prompt.trim()) return;
          setIsSubmitting(true);

          const { notes, facilitatorFeedback } = await sendToEvaluator({ prompt });
          const ts = new Date().toISOString();
          const newRec = {
            id: crypto.randomUUID(),
            ts,
            firstname: firstName.trim(),
            lastname: lastName.trim(),
            prompt,
            notes,
            facilitatorfeedback: facilitatorFeedback
          };
          console.log(newRec);
          if (USE_LOCAL_STORAGE) {
            saveRecordLocal(newRec);
            setRecords(prev => [newRec, ...prev]);
          } else {
            // Save to backend (now using prompts table in promptdb)
            try {
              await fetch('/api/record', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify(newRec)
              });
            } catch (err) {
              alert('Failed to save record to server.');
            }
            setRecords(prev => [newRec, ...prev]);
          }
          setLastEval({ notes, ts });
          setPrompt(""); setFirstName(""); setLastName("");
          setIsSubmitting(false);
        };

        return (
          <div className="w-full pt-20 pb-12 px-4">
            <div className="flex items-start max-w-7xl mx-auto">
              <div className="flex-shrink-0 w-[160px] mr-8 flex justify-center">
                <img src="./resources/tm_logo.png" alt="Thinking Machines Logo" className="h-[160px] w-auto" />
              </div>

              <div className="flex-1 max-w-5xl mx-auto">
                <header className="mb-6 text-left">
                  <h1 className="text-4xl font-bold leading-tight mb-2">Thinking Machines Prompt Evaluator</h1>
                  <span className="text-sm text-gray-500 hidden sm:inline">Beta testing</span>
                  <p className="text-gray-600 mt-2">Quickly score &amp; review prompts during enablement sessions.</p>
                </header>

                <section className="mb-10">
                  <form onSubmit={handleSubmit} className="bg-white shadow rounded-lg p-10 space-y-8">
                    <div>
                      <label className="block text-sm font-medium mb-1">First Name <span className="text-red-600">*</span></label>
                      <input type="text" className="w-full border rounded px-5 py-3" value={firstName} onChange={e => setFirstName(e.target.value)} placeholder="e.g., Alice" required />
                    </div>
                    <div>
                      <label className="block text-sm font-medium mb-1">Last Name <span className="text-red-600">*</span></label>
                      <input type="text" className="w-full border rounded px-5 py-3" value={lastName} onChange={e => setLastName(e.target.value)} placeholder="e.g., Garcia" required />
                    </div>
                    <div>
                      <label className="block text-sm font-medium mb-1">Prompt <span className="text-red-600">*</span></label>
                      <textarea className="w-full border rounded px-5 py-3 h-40" value={prompt} onChange={e => setPrompt(e.target.value)} placeholder="Paste your prompt here to check against prompt writing best practices" required />
                    </div>
                    <div className="flex items-center gap-4">
                      <button type="submit" disabled={isSubmitting || !prompt.trim()} className={`px-7 py-3 rounded text-white ${isSubmitting || !prompt.trim() ? 'bg-gray-400 cursor-not-allowed' : 'bg-blue-600 hover:bg-blue-700'}`}>{isSubmitting ? 'Evaluating…' : 'Submit'}</button>
                      <span className="ml-auto text-sm text-gray-500 hidden sm:inline">Stored locally for admin view</span>
                    </div>
                  </form>

                  {lastEval && (
                    <div className="mt-10 bg-white border-l-4 border-blue-500 shadow rounded-lg p-8 fade-in">
                      <h3 className="font-semibold mb-2 text-base">Evaluation Feedback <span className="text-gray-500">({new Date(lastEval.ts).toLocaleString()})</span></h3>
                      <p className="whitespace-pre-wrap break-words text-sm">{lastEval.notes}</p>
                    </div>
                  )}

                  {/* Facilitator Feedback is not shown to the user */}
                </section>

                <footer className="mt-20 text-xs text-gray-400 text-center">
                  <p>POC only – using live OpenAI API evaluation. Store your API key securely.</p>
                </footer>
              </div>

              <div className="flex-shrink-0 w-[160px] ml-8"></div>
            </div>
          </div>
        );
      }

      ReactDOM.createRoot(document.getElementById('root')).render(<App />);
    </script>
  </body>
</html>
