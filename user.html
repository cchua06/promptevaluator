<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>GenAI Prompt Feedback POC</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script crossorigin src="https://unpkg.com/react@18/umd/react.development.js"></script>
    <script crossorigin src="https://unpkg.com/react-dom@18/umd/react-dom.development.js"></script>
    <script src="https://unpkg.com/@babel/standalone/babel.min.js"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
    <style>
      body { @apply bg-gray-50 text-gray-800; font-family: 'Inter', sans-serif; }
      textarea { font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace; }
      .fade-in { animation: fade-in 0.3s ease-out; }
      @keyframes fade-in { from { opacity:0; transform: translateY(4px);} to { opacity:1; transform: translateY(0);} }
      h1, h2, h3, h4, h5, h6, .font-bold, .font-semibold, .text-3xl, .text-4xl, .font-bold, .font-semibold {
        font-family: 'Inter', sans-serif;
        font-weight: 600;
      }
    </style>
  </head>
  <body class="min-h-screen flex flex-col">
    <div id="root" class="flex-1"></div>

    <script type="text/babel">
      // ---------------------------
      // OpenAI Evaluator Function
      // ---------------------------

      async function evaluatePrompt(prompt) {
        // System instructions for the evaluator
        const systemInstructions = `
        ðŸ“¥ SYSTEM PROMPT: Prompt Structure Evaluator

        You are a prompt reviewer. Your task is to evaluate whether a userâ€™s prompt includes all 4 required components:

        Task: Clear action with a specific deliverable and audience.

        Input: Raw materials, context, or data sources the AI should use.

        Steps: A numbered, sequential thought process (with action verbs).

        Output: Clear format, tone, and structural expectations.

        ðŸ” Instructions:
        Parse the userâ€™s prompt carefully.

        For each component, mark as:

        âœ… Present

        âŒ Missing
        If present but weak (vague, incomplete, or lacking best practices), flag it.

        Then produce a brief structured summary using the appropriate output format.

        ðŸ“‹ Output Format:
        If Failed:

        Letâ€™s try again! ðŸ¤“

        Missing components: [List any of: Task, Input, Steps, Output]

        Improvements needed in: [List components that are present but need strengthening]

        Breakdown:
        Task: [If weak or missing, explain briefly why]
        Input: [If weak or missing, explain briefly why]
        Steps: [If weak or missing, explain briefly why]
        Output: [If weak or missing, explain briefly why]

        If Successful:

        Amazing job! ðŸŽ‰

        All 4 components are present!

        Best practices demonstrated in: [List highlights such as â€œclearly scoped inputâ€, â€œwell-sequenced stepsâ€, â€œprecise output formattingâ€]

        âœ… Evaluation Criteria Recap:
        Task: Starts with a verb, includes a defined deliverable and target audience.

        Input: Specific and bounded; includes background, data, or cited sources.

        Steps: Numbered, uses action verbs, includes decisions or clarifications where needed.

        Output: Provides template-level clarity on tone, length, structure, and format.

        â›”ï¸ Do not suggest a revised prompt.
        â›”ï¸ Do not include a question or ask if the user wants help.
        â›”ï¸ Do not end with guidance or speculative improvements.
        â›”ï¸ Do not include any instruction text or headings in your output â€” only use the approved output format above.

        Keep your evaluation concise and focusedâ€”only highlight what matters most.
        `;

        try {
          const res = await fetch('/api/evaluate', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({ prompt, systemInstructions })
          });
          if (!res.ok) {
            throw new Error(`API error: ${res.status} ${res.statusText}`);
          }
          const data = await res.json();
          return { notes: data.notes };
        } catch (err) {
          console.error('Error evaluating prompt:', err);
          return { notes: 'There was an error contacting the evaluation service.' };
        }
      }

      const isBlank = (s) => !s || s.trim().length === 0;
      // Call backend to generate Facilitator Feedback using OpenAI GPT-4o
      const generateFacilitatorFeedback = async (prompt, evaluation) => {
        const formattedInput = `Prompt:\n${prompt}\n\nEvaluation:\n${evaluation}`;
        const systemInstructions = `
        Your task is to generate structured support content to help the facilitator guide the participant toward a better prompt.

        Inputs:
        Prompt:
        <prompt>

        Evaluation:
        <evaluation>

        Your output must follow this format:

        Task Summary:

        Summarize what the participant is trying to achieve based on their prompt. Use the evaluation only to clarify whatâ€™s missing or weakâ€”donâ€™t just restate it. Write 1â€“2 objective sentences. If the task is unclear, say so.

        Guidance:

        Write 2â€“5 bullet points that provide facilitator-facing guidance. These should be confident, actionable suggestionsâ€”not questions. Focus on moving the participant forward based on whatâ€™s wrong, as indicated in the evaluation. Avoid direct criticism or prompt rewrites.

        Be use-case specific when possible: e.g., suggest uploading a file, clarifying audience, giving example inputs, or breaking the task into steps.

        Always include this as the final bullet:
        - Ensure there's a source: an online link, uploaded file (e.g. agenda, program), or concise input directly in the prompt.
        `;

        try {
          const res = await fetch('/api/facilitator-feedback', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({ prompt, systemInstructions })
          });
          if (!res.ok) {
            throw new Error(`API error: ${res.status} ${res.statusText}`);
          }
          const data = await res.json();
          return data.feedback || '';
        } catch (err) {
          console.error('Error generating Facilitator Feedback:', err);
          return 'There was an error generating the facilitator feedback.';
        }
      };

      const sendToEvaluator = async ({ prompt }) => {
        const evaluationFeedback = await evaluatePrompt(prompt);
        const { notes } = evaluationFeedback;
        const facilitatorFeedback = await generateFacilitatorFeedback(prompt, evaluationFeedback.notes);
        return { notes, facilitatorFeedback };
      };


      // ---------------------------
      // Storage Toggle
      // ---------------------------
      let USE_LOCAL_STORAGE = false; // Will be set by backend

      const STORAGE_KEY = 'prompt_records_v1';

      function saveRecordLocal(rec) {
        const arr = JSON.parse(localStorage.getItem(STORAGE_KEY) || '[]');
        arr.unshift(rec);
        localStorage.setItem(STORAGE_KEY, JSON.stringify(arr));
      }
      function loadRecordsLocal() {
        try { return JSON.parse(localStorage.getItem(STORAGE_KEY) || '[]'); } catch { return []; }
      }

      // ---------------------------
      // React Component
      // ---------------------------
      function App() {
        const [firstName, setFirstName] = React.useState("");
        const [lastName,  setLastName]  = React.useState("");
        const [prompt,    setPrompt]    = React.useState("");
        const [records,   setRecords]   = React.useState([]);
        const [isSubmitting, setIsSubmitting] = React.useState(false);
        const [lastEval,  setLastEval]  = React.useState(null);
        // Facilitator Feedback is not shown to the user

        // On mount, get storage mode and load records if local
        React.useEffect(() => {
          fetch('/api/storage-mode').then(r => r.json()).then(({ useLocalStorage }) => {
            USE_LOCAL_STORAGE = useLocalStorage;
            if (USE_LOCAL_STORAGE) setRecords(loadRecordsLocal());
          });
        }, []);

        // For localStorage, keep in sync
        React.useEffect(() => {
          if (USE_LOCAL_STORAGE) localStorage.setItem(STORAGE_KEY, JSON.stringify(records));
        }, [records]);

        const handleSubmit = async (e) => {
          e.preventDefault();
          if (isBlank(firstName) || isBlank(lastName)) return alert("First and last name cannot be blank.");
          if (!prompt.trim()) return;
          setIsSubmitting(true);

          const { notes, facilitatorFeedback } = await sendToEvaluator({ prompt });
          const ts = new Date().toISOString();
          const newRec = {
            id: crypto.randomUUID(),
            ts,
            firstname: firstName.trim(),
            lastname: lastName.trim(),
            prompt,
            notes,
            facilitatorfeedback: facilitatorFeedback
          };
          console.log(newRec);
          if (USE_LOCAL_STORAGE) {
            saveRecordLocal(newRec);
            setRecords(prev => [newRec, ...prev]);
          } else {
            // Save to backend (now using prompts table in promptdb)
            try {
              await fetch('/api/record', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify(newRec)
              });
            } catch (err) {
              alert('Failed to save record to server.');
            }
            setRecords(prev => [newRec, ...prev]);
          }
          setLastEval({ notes, ts });
          setPrompt(""); setFirstName(""); setLastName("");
          setIsSubmitting(false);
        };

        return (
          <div className="w-full pt-20 pb-12 px-4">
            <div className="flex items-start max-w-7xl mx-auto">
              <div className="flex-shrink-0 w-[160px] mr-8 flex justify-center">
                <img src="./resources/tm_logo.png" alt="Thinking Machines Logo" className="h-[160px] w-auto" />
              </div>

              <div className="flex-1 max-w-5xl mx-auto">
                <header className="mb-6 text-left">
                  <h1 className="text-4xl font-bold leading-tight mb-2">Thinking Machines Prompt Evaluator</h1>
                  <span className="text-sm text-gray-500 hidden sm:inline">Beta testing</span>
                  <p className="text-gray-600 mt-2">Quickly score &amp; review prompts during enablement sessions.</p>
                </header>

                <section className="mb-10">
                  <form onSubmit={handleSubmit} className="bg-white shadow rounded-lg p-10 space-y-8">
                    <div>
                      <label className="block text-sm font-medium mb-1">First Name <span className="text-red-600">*</span></label>
                      <input type="text" className="w-full border rounded px-5 py-3" value={firstName} onChange={e => setFirstName(e.target.value)} placeholder="e.g., Alice" required />
                    </div>
                    <div>
                      <label className="block text-sm font-medium mb-1">Last Name <span className="text-red-600">*</span></label>
                      <input type="text" className="w-full border rounded px-5 py-3" value={lastName} onChange={e => setLastName(e.target.value)} placeholder="e.g., Garcia" required />
                    </div>
                    <div>
                      <label className="block text-sm font-medium mb-1">Prompt <span className="text-red-600">*</span></label>
                      <textarea className="w-full border rounded px-5 py-3 h-40" value={prompt} onChange={e => setPrompt(e.target.value)} placeholder="Paste your prompt here to check against prompt writing best practices" required />
                    </div>
                    <div className="flex items-center gap-4">
                      <button type="submit" disabled={isSubmitting || !prompt.trim()} className={`px-7 py-3 rounded text-white ${isSubmitting || !prompt.trim() ? 'bg-gray-400 cursor-not-allowed' : 'bg-blue-600 hover:bg-blue-700'}`}>{isSubmitting ? 'Evaluatingâ€¦' : 'Submit'}</button>
                      <span className="ml-auto text-sm text-gray-500 hidden sm:inline">Stored locally for admin view</span>
                    </div>
                  </form>

                  {lastEval && (
                    <div className="mt-10 bg-white border-l-4 border-blue-500 shadow rounded-lg p-8 fade-in">
                      <h3 className="font-semibold mb-2 text-base">Evaluation Feedback <span className="text-gray-500">({new Date(lastEval.ts).toLocaleString()})</span></h3>
                      <p className="whitespace-pre-wrap break-words text-sm">{lastEval.notes}</p>
                    </div>
                  )}

                  {/* Facilitator Feedback is not shown to the user */}
                </section>

                <footer className="mt-20 text-xs text-gray-400 text-center">
                  <p>POC only â€“ using live OpenAI API evaluation. Store your API key securely.</p>
                </footer>
              </div>

              <div className="flex-shrink-0 w-[160px] ml-8"></div>
            </div>
          </div>
        );
      }

      ReactDOM.createRoot(document.getElementById('root')).render(<App />);
    </script>
  </body>
</html>
